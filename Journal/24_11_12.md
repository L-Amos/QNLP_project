# Day 10: 12th November 2024
## Entry 1: 09:19
This week I have been swamped by deadlines, but I have managed to carve out today to do some solid work on this project. I have several goals in mind for today:

- Sort out the file structure of my repo.
- Write a function to determine the accuracy of my model.
- Rather than randomly generating train, test and validation data, specify the generation to generate **good** data, constituted by the following parameters:
    - 50% of pairs having sentences of the same topic and 50% of different topics
    - Every word in the vocabulary used at least three times
    - A good range in SBERT cosine similarity
- Once this data is generated, conduct my first **real and rigorous** investigation into model training, plotting convergences for different hyperparameters.
- Conduct a similar investigation with the default binary classification model, and compare the two models' convergence (hopefully my model will be much better).
- For the above two investigations to be feasible, I may need to add a flag to use a simpler simulator rather than thr Aer simulator.
- Time allowing, place these Journal pages into a Sphinx-created website + host on GitHub pages.

As you can see, I have a lot to be getting on with, so I better get cracking!

## Entry 2: 17:54
Today's been one of those days with failure after failure.

I managed to fairly quickly generate some good*ish* data with a new Python script. I was able to make sure 50% of the pairs have the same topic and 50% different, and I pick 
these pairings randomly, meaning statistically I am likely to have a good range in SBERT cosine similarity. However, I was unable to ensure each word in the vocabulary appeared 3 times. This is because **training was intolerably slow**. Therefore, the larger the training dataset, the longer I had to wait to see the results of my training. At one point it was taking minutes to do *single epochs*, which clearly isn't fast enough for this testing stage.

Thus, I have spent most of the day trying desparately to improve the training times. I initially rewrote everything in tket, thinking perhaps there was some behind-the-scenes jitting going on which I could exploit. Unfortunately, this had no effect. Switching back to Qiskit, I tried a variety of different methods of increasing the simulation speed, including switching from a DisCoCat model to a bag-of-words model. However, while this made the generation of the PQCs much faster, the actual difference in training time was negligible.

I have finally found a solution: qiskit can be run on the GPU with **a lot of faff**. I am running an Ubuntu instance in WSL with altered code to get it to work, but it is up to 3x faster, which is much more reasonable. I have also vastly reduced the training dataset size from 80 individual sentences and 160 pairs to 40 individual sentences and 80 pairs. Now each epoch only takes ~15 seconds to run through, with validation of 20 sentences and 40 pairs taking only a couple of seconds.

I have now begun to obtain data, running through 60 epochs to give a general idea of the performance of the trainer. Training will likely take around 15-20 minutes to complete for each set of hyperparameters. I will start by changing the hyperparameters systematically and plotting how the train and test losses change.

Once I have done this, I will run the same investigation (with the same hyperparameters) using the binary classification model, rather than the fidelity model. I will also implement an accuracy function for early-stopping, however this is less pressing. What is important today is to get some real data to send to Prof. Sadrzadeh.