# Day 2: 9th October 2024
## Entry 1: 09:49
Today I want to play around with lambeq. Specifically, I want to train a model using the training data from Lorentz. et al [^1][^2] and then poke and prod that model to see what happens. Specifically, I want to test the model with a sentence very similar to one of its training sentences, then implement the state fidelity circuit [I got working last time](/testing/simple_swap_test.ipynb) to see just how close the two states are. If they're miles apart, this whole QKNN thing won't work, and I'll have to alter the training phase somehow.

I'll be using binary classification for now, as I need a simple approach. I tried to get Lorentz et al's code[^3] working, but as it was written 3-4 years ago packages and libraries have been updated - I think it would be more beneficial to start from scratch with the latest versions and methods of the various packages.

## Entry 2: 11:35
I am working in the notebook located [here](/testing/training_new_model.ipynb). Had issues with the training - everything was absolutely fine until I tried training the model. I was getting some sort of odd error: `KeyError: ResultHandle('1d68d2f5-60a4-48af-b771-51637cd791b3', 1, 3, 'null')` or something similar. I decided to download the notebook [here](https://cqcl.github.io/lambeq-docs/tutorials/trainer-quantum.html) and try and run it without editing. It had the same error. After much tinkering, it turns out, as outlined in [this GitHub issue](https://github.com/CQCL/lambeq/issues/153), that the problem is to do with the `pytket` module - it has been updated to be incompatible with the lambeq trainer in some way. Downgrading to v0.50.0 fixed the issue, and I can now run the downloaded notebook without issue.

The downloaded notebook deals with classifying relative pronouns, which although a binary classification task is not exactly what I want, since it classifies more based on the syntax of the sentence rather than the meaning of the sentence. 

Now that I have this notebook working, however, I will try and return to my own notebook and run the trainer.

## Entry 3: 12:59
It took nearly half an hour but I have a trained model. I can now use that model to predict states of qubits after being run through new circuits derived from unseen sentences. I am now able to play with this model, to see how well it performs, and to see if it can be used for QKNN purposes.

[^1]: https://arxiv.org/pdf/2102.12846
[^2]: https://github.com/CQCL/qnlp_lorenz_etal_2021_resources/tree/main/datasets/ 
[^3]: https://github.com/CQCL/qnlp_lorenz_etal_2021_resources