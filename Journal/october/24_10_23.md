# Day 6: 23rd October 2024
## Entry 1: 11:49
Quick one today; I wasn't planning on doing any work today, but I had an idea I'd quite like to try out.

The current problem I'm working on is finding a good way to evaluate my fidelity test (i.e to test if it actually rates sentence similarity in a useful way). The current idea I have is to use a BERT model to run through each sentence in the test array and for each sentence rank all the sentences in the train array by similarity. I can then perform the same thing with my quantum swap test circuit, and then compare the ranked arrays. This is essentially creating a *classical* recommender system to evaluate the fidelity test before moving on to creating a *quantum* recommender system. Since I have so many investigations planned for this fidelity test circuit, finding a good way to evaluate its performance is essential.

The manner in which I compare the ranked arrays is up for debate, but [NDCG](https://www.geeksforgeeks.org/normalized-discounted-cumulative-gain-multilabel-ranking-metrics-ml/) appears to be a pretty ubiquitous metric for such a comparison, so I'll probably use that. My primary goal for now is to get a BERT model downloaded and spitting out similarities.

## Entry 2: 22:30
I haven't been working on this all day; I got busy with lectures etc. Thankfully tomorrow I have a big window during the day which I am excited to devote to this project. Today I did manage to write [a bit of code](https://github.com/L-Amos/QNLP_project/blob/fidelity_evaluation/src/fidelity_accuracy_evaluator.py) playing around with fidelity accuracy evaluation. I found that the first sentence in the test dataset fails miserably on the quantum pipeline; fidelity is very high for sentences of the complete wrong class. This may be a case where the accuracy of the model fails, since running the sentence through the model gives the output $\approx [1/3, 2/3]$, whereas the expected output is $[0, 1]$, meaning the accuracy isn't great. Thankfully the second sentence in the test dataset seemed to yield better results (subjectively speaking of course). Since the dataset used to train the model is small, there are bound to be failures such as the first sentence - what I need to know is exactly *how many* failures there are.

Tomorrow I will focus on implementing the NDCG calculation to quantifiably evaluate the accuracy of my quantum pipeline against the BERT model. I will calculate a value for each sentence in the test dataset, and average these out to get an overall measure of how close my quantum implementation is to a classical BERT implementation. This should give me a much better idea of how well my pipeline works in the state it's in. I look forward to it!