# Day 13: 15th January 2025
## Entry 1: 09:30
After a long break away (due to coursework assignments and christmas) I am eager to get back to work on the project. In my timeline for the project, I wanted to have a fully-trained fidelity model by the end of January, and I will work hard to try and meet this deadline.

Today I will train my model, systematically altering the hyperparameters to try and train a good fidelity model. To increase the likelihood of good training, I will double the amount of training data. This is possible now due to the vastly reduced training times courtesy of the new numpy model I am using. My plan is as follows:

- Double the training and validation data
- Find a way to implement early stopping to avoid over-fitting
- Train a fidelity model using a bag-of-words approach to syntax
    - Pick some arbitrary starting point for the hyperparameters
    - Train the model for a fixed number of epochs saving training and validation costs for each epoch
    - Save the results as a graph of cost vs. training epoch
    - Incrementally increase/decrease the hyperparameters, training for each set 
    - Once all training is done, visually compare the graphs to find the best hyperparameters
- Repeat for a number of different language models (word-sequence, discocat, etc.)

Before I look into that, though, I need to see if I can use a better SBERT similarity quantity other than the cosine similarity. There may be a similarity which is more analogous to the fidelity between two quantum states; I will look into that first.

## Entry 2: 16:30
I have managed to alter my train script to systematically change the learning rate hyperparameter from 0 to 1 in increments of 0.05 across 240 epochs, and have saved the train and validation costs for each training run. I have also doubled the amount of training and validation data, and have changed the method of data generation to use the SBERT **dot product** similarity instead of the cosine similarity. Since I have explicitly set the model to generate *normalised* embeddings, the dot product similarity will be between 0 and 1, and therefore better match up with the potential fidelity measurement (which also ranges from 0 to 1). 

However, I cannot help but think I am missing something hugely important; before next week I will read everything I can about machine learning and QML to try and understand if my approach to training this model is the best one.