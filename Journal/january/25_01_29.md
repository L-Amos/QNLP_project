# Day 14: 29th January 2025
## Entry 1: 09:19
It's been a frustratingly-long time since I have had a chance to work on this project, so for the next few days I am totally committed. 

As the number of approaches I try inflates, and the number of changes I make to the program grows, I need to keep my current workflows in my mind both to enable me to write a thesis effectively and to ensure I have good reasoning for taking the steps I am. Therefore, today I will explicitly write out my current approach in its entirety, as this will solidify my reasoning in my mind and expose any hand-waving I am currently doing. In particular, I wish to revisit these core parts of my project:
- Research ansatze + provide reasoning for why I have chosen the one I have chosen
- Same as above for optimisers
- Same as above for cost functions
- Identify a way of comparing trained models to tune hyperparameters

## Entry 2: 23:03
Excellent progress!!! I have some very exciting results to share.

Firstly, I have switched from the IQP ansatz to the Strongly Entangled ansatz. This is because in a number of papers (e.g [here](https://link.springer.com/article/10.1007/s00521-023-08706-7) and [here](https://www.sciencedirect.com/science/article/pii/S0957417424012934)) have found that this ansatz converges faster than the IQP ansatz for QNLP text classification. 

I also have a new approach for understanding the efficacy of my model; I am trying to, essentially, represent sentences such that their states form *clusters* on the Bloch sphere corresponding to the relevant categories. So the simplest way to visualize whether this is working is to *plot the states on a Bloch sphere*. If the states form clusters, then the model is working. If they are random, I know I need to do some more work.

I also realised that the amount of training data was too few; I increased the number of training pairs to 500 and the number of validation pairs to 100. I also made sure that these pairs were selected *wholly randomly*; since I am creating far more pairs than there are sentences (there are 80 test sentences) it is likely I will have an even distribution of pairs of sentences of the same category and pairs of sentences of different categories. Thankfully, training time is still tractable with the simplistic sentences I am working with. Furthermore, I am now *averaging* over 5 training runs, as there was some variance between runs (for the official experiments I will average over more runs, say 20, but while I'm tuning hyperparameters 5 should suffice).

I found that a learning rate of 0.45 gave relatively quick convergence, and so I trained this model for 120 epochs, then ran each of the train sentences through the model and obtained their quantum states. I plotted these states on a bloch sphere, colouring them depending on their category (food or I.T), and the results are shown below.

![TRAIN_a=0 45_120_epochs_500train_100val](https://github.com/user-attachments/assets/8a9f745c-8fe2-4b67-9600-3e143d5a0793)

In this diagram, the red states represent sentences concerning I.T, and the blue states represent sentences concerning food. There are two clusters, which is promising, however they are not well-separated, and so would be poor for a KNN algorithm. More training, however, may help with this - this will be a focus of next session.

After this, I realised that I could separate the clusters by altering the training labels. Instead of having the training levels be the SBERT similarity between them, I realised I could instead manually pick the labels: if the two sentences were of the same category, their label would be high (0.9), and if they were of different categories their label would be low (0.1). This ensured that sentences of the same category would have *higher state fidelities*, and thus be close together on the Bloch sphere. The inverse would be true of sentences of different categories. Training for 120 epochs (again, with a learning rate of 0.45) gave the following Bloch sphere:

![a=0 45_alternative_120_epochs_500train_100val](https://github.com/user-attachments/assets/876d671d-e708-451d-86e9-4bb0f110c67f)

In this diagram, the red states represent sentences concerning I.T, and the blue states represent sentences concerning food. Clearly, there are two distinct clusters which are well-separated, making them good for KNN purposes. I then ran 23 *unseen* sentences through the model and plotted their states on the Bloch sphere, obtaining the following result:

![TEST_a=0 45_alternative_120_epochs_500train_100val](https://github.com/user-attachments/assets/d9779e3a-953c-4184-a084-a5d207530a42)

The orange states represent sentences concerning I.T and the purple states represent sentences concerning food. Happily, the test sentences cluster in the same way to the train sentences, indicating they will be correctly classified with a KNN algorithm. Happy days!

However, while this is good for classification, rating the sentences in terms of 'semantic similarity' may no longer work, as semantic similarity is now being *wholly* interpreted by the model, with no guidance from the training labels. This is an avenue to explore.

Thus, I have the following experiments in mind, once I find optimum hyperparameters. Note, the below concerns only *binary classification*.
1. Perform the above experiment *officially*, showing states on the Bloch sphere, for both types of training label, highlighting the difference between each.
2. Implement classical KNN for sentence classification, and find out which type of training label yields better results.
3. Perform the above two experiments for a *more complex* set of data, with longer sentences and a larger vocabulary, to see which model scales best.

The above can be repeated for multi-class classification as well, as well as the final 'holy grail' of recommendation. Thus, I will be working with four distinct combinations of models and datasets:
1. Simple sentences, similarity labels.
2. Simple sentences, arbitrary labels.
3. Complex sentences, similarity labels.
4. Complex sentences, arbitrary labels.

Also, I have been using DisCoCat so far exclusively, so it would be nice to try other methods such as bag-of-words and word-sequence, comparing each of these models to see which is best. All in all, there are a lot of experiments I can do with a lot of data to collect and a lot of interesting comparisons to be made.

This gives good structure for the project report, and a good coverage of the potential ways of using my model. Hopefully, my experiments will yield a good way of training a model which can be applicable to classification and recommendation, and scaled to more complex datasets.
