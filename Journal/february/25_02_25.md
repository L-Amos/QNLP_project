# Day 16: 25th February 2025
## Entry 1: 09:13
Frustratingly, another three weeks have gone by with absolutely no progress being made on this project. This is due to various reasons keeping me at home, where I can't work because my laptop isn't good enough to run the training in a tractable amount of time.

As a result, today will be **heavy**. By the end of the day I want to have:
- For each language model (bag-of-words, DisCoCat etc.):
    - Fully-trained a model on my simple sentence dataset using both similarity and arbitrary labels.
    - Shown the Bloch diagram for the test sentence states, to show the level of clustering achieved.
    - Implement classical KNN for binary sentence classification to find out which combination of model and label type yields the best results.
- Create a set of more complex sentences (still with binary labels) and repeat the above with these sentences to see how well the model scales
- Create a set of simple and complex sentences with *multi-class* labels so that the above can be repeated for *multi-class* classification in the future.

Clearly, there is a lot to be done. I sense an all-nighter coming on...

## Entry 2: 15:19
Lots of progress has been made.

Firstly, I found an error: when visualizing states on the Bloch sphere, I had been using `model._normalize_vector` to normalise the sentence states outputted by the model. However, this function returns the *probabilities of measuring 0 or 1, not the state vector*. Therefore I was plotting the wrong states. Thankfully, I caught this issue now before too much harm was done - the solution was ot use `np.linalg.norm` to re-normalise the statevectors.

I have added support for the bag-of-words and word-sequence models, and am training fidelity models with these models of language to compare against the DisCoCat model. I have also implemented a classical KNN algorithm which I have used to classify the test sentences using my fidelity model. The results are extremely interesting, but I will wait until I have rigorously and neatly written them up before alluding to them...