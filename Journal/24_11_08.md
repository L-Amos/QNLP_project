# Day 9: 8th November 2024
## Entry 1: 12:45
After a very busy week of weddings and coursework deadlines, I am ready to get back to work on this project. Very little to say; my main focus today will be to try and build a quantum trainer which uses *fidelity measurement* as a loss metric.

## Entry 2: 23:36
Been working on-and-off on this today. Finally managed to accomplish what I set out to; the key was to **build a new model**, not a new trainer. My new model is identical to a `TketModel` from the lambeq package, except its `forward()` method adds a swap circuit and uses Qiskit's Aer backend to simulate the circuit and measure the fidelity, including post-selection. This was done by copying and pasting the code I had already written for this task, as can be seen in [the SitRep](./SitRep-29-10-24.ipynb) written last week.

I have trained my model with a variety of hyperparameter values and have plotted its convergence, as can be seen on the other branch of this repo. It seems to be working OK, but my training data today only consisted of 30 pairs of sentences, and I only ran the model for 120 epochs (as it took ~20 minutes to train for this long). I am merely trying to prove a concept here - I will leave the rigorous investigations until after I have a rough working prototype. Tomorrow I will take the best hyperparameters I found today ($a=0.3, c=0.06$) and train the model on a larger dataset (100 sentence pairs) for longer (at least 300 epochs). Hopefully the model will converge to a low loss value. Once I have this model, I will try briefly evaluating its NDCG on some test sentence pairs in the same way as is described in the SitRep above.