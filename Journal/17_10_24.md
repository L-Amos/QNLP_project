# Day 4: 17th October 2024
## Entry 1: 03:28
Please excuse the early hour, but I couldn't sleep and it seemed like a good idea to get cracking with this week's project work. 

I have managed to organise my thoughts regarding the use of QKNN with lambeq. I thought that I would need to have access to the quantum circuits with optimum parameters found by the model in order to compare qubit state fidelity. However, this is not necessarily the case, as I can simply store the states returned by the model classically, and then re-encode them onto qubits before using the QKNN algorithm. However, there is an issue which I found when playing with my [state fidelity circuit](../testing/state_fidelity_with_model.ipynb): the training phase of the lambeq pipeline includes post-selection, which means the returned qubit states are **sub-normalized**. This means they cannot be re-encoded onto qubits.

Today's job is to use a [2022 Oxford PhD thesis](https://www.cs.ox.ac.uk/people/aleks.kissinger/theses/khatri-thesis.pdf) to find an approach to work around the sub-normalization in a way which allows for my state fidelity circuit to be implemented. Failing that, I could form a theoretical framework for using [NV sensors](https://www.nature.com/articles/s41467-017-00964-z) to measure the qubits in the training phase in a single-shot, removing the need for post-selection. However, since current quantum computers do not use NV sensors, this would not be experimentally verifiable, and would require lots of work to simulate. It may be an interesting extension to the project regardless.

## Entry 2: 04:54
Exciting things have happened! 

In order to work around the sub-normalization issue, I simply normalized the qubit states using numpy's `linalg.norm` method. ***I will need to check that this is allowed***. However, in the mean time, I think I have a working fidelity testing circuit!

At first I thought there was an issue; the fidelity appeared to be roughly the same regardless of whether the two sentences parsed were similar or completely different. In some cases, the two different sentences yielded higher fidelity than the two similar ones! After messing around with an iterative function to average out the states returned by the model, I realised I had initialized the states into qubits 0 and 1, whereas they needed to be initialized to qubits 1 and 2, with qubit 0 being in the $|0\rangle$ state, [as shown in this paper](https://arxiv.org/pdf/2003.09187). Thankfully I realised my issue and after changing things I was consistently getting higher fidelity for similar sentences than different ones, [as shown in this notebook](../testing/state_fidelity_with_model.ipynb). I have now created a [Python script](../src/state_fidelity_test.py) which contains functions to do the sentence parsing and fidelity test.

Now that I know this part of the algorithm works, I feel much more secure in the project; I can clearly see how I would extend what I've done into a full QKNN recommender system. I could even conduct a brief investigation into what makes the fidelity change most (changing pronouns, adding adjectives, etc.). With that in mind, I propose the following tentative roadmap for the whole project:

In simulations:
- Train a lambeq model with the data from [the lambeq docs](https://github.com/CQCL/lambeq-docs/tree/main/docs/examples/datasets) and test its accuracy to ensure it matches that found by [Lorentz et. al](https://arxiv.org/pdf/2102.12846).
- Implement a QKNN recommender system utilizing the fidelity test to recommend the 5 most similar sentences to a given input sentence.
- Find some way to quantitatively measure the performance of the recommender system (look into it)
- Create my own dataset with different categories (e.g sport and music) and repeat the above, measuring both the model's accuracy and the performance of the recommender system
- Create a data set with **multiple classes** and repeat the above, measuring both the model's accuracy and the performance of the recommender system
- Perform the same experiments on a real NISQ device.
- If time allows, explore what kinds of changes to a sentence have the biggest impact on fidelity (e.g adding words, changing pronouns etc.)

## Entry 3: 11:43
No, I haven't slept yet.

I decided to extract the circuits from lambeq and use them directly, rather than embedding the states spat out by the model. This turned out to be very interesting because it doesn't actually work; two identical sentences have a fidelity of 1, as expected, but 2 similar sentences may have a fidelity of less than 1/2, and sometimes two different sentences will have a fidelity of 1. It is quite frustrating. I will look into why this may be happening at a later date; I imagine I wrote a bit of code wrong and given that I haven't slept in about 30 hours I doubt looking over it again will help. 

For now, I have two branches in this repo: one with a normalization approach and one with a retrieve-and-compose-circuits approach. 