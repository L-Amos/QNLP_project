# Day 19: 18th May 2025
## Entry 1: 09:30
Today I will be working in between revision, so I won't be able to fully focus on what I need to do. What I want to do is see if increasing the amount of train data has a noticeable effect on convergence. This need to be done before the other experiments - I don't want to spend a long time optimising parameters for 500 train pairs when a greater number of train pairs works much better.

On a positive note, I'm not sure what I was talking about in the last entry RE the correlation coefficient - the correlation is actually very good! After training it goes from near zero (as expected for randomly-initialised rotation gates) to 0.7-0.8, showing a strong positive correlation! This is very promising.

## Entry 2: 11:23
After a small amount of testing I don't think the amount of train data is making much difference. I tried similarity labels with 1000 train pairs and 200 validation pairs (both double what I was using before) and it doesn't seem to have made much meaningful difference. I'm running a more rigorous experiment just in case (learning rate 1, 120 epochs, avg. over 5 training runs), but I think I can disregard trying to increase the length of my training data. Which is good, because I'd need more RAM if I had more data.

While those experiments are running, this repo has gotten bloated again. What I really need to do is the following:
1. Clean up the 'numpy-model' branch and structure it like an actual module.
2. Move all experimental data into its own 'experiments' branch.
3. Merge 'numpy-model' into main.
4. Add a toml to make my package importable, making every script much neater.
5. Add various READMEs to make understanding my model + the module much easier.