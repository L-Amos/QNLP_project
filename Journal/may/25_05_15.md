# Day 18: 15th May 2025
## Entry 1: 13:55
Not much time today, only a couple of hours. I've noticed that JAX can be configured to run on the GPU. This may seriously help with my training times - that is what I'll investigate now.

## Entry 2: 16:13
GPU is a no-go. I got it working, but it turned out to be slower than the CPU - I assume the overhead in moving data to and from the GPU is too great. I also dabbled in trying to save the pre-compiled functions to get rid of that initial compilation time, but that doesn't work. I suspect it's because every rotational gate is initialised to a random weight, meaning every time a new model is initialised you're technically starting with completely different circuits, so you need to compile them again. 

So it's back to the drawing board. I have managed to calculate the Pearson correlation coefficient between the train labels and my model's outputted fidelities. It's not good - 0.3ish for my best runs so far. Having a look at the convergences, they're not good - they stick at a MSE of something like 0.1, which corresponds to a MAE of 0.3. We'd expect the average error to be around 0.5, as fidelity is bounded between 0 and 1. So training does improve things, but only slightly. 

Over the next few days I need to take stock and work out why the convergence reaches such a high plateau, and what can be done to decrease it. I suspect the secret lies in the binary label convergences - when using binary labels, I converge to close to 0, which is excellent. It's easier to find lots of states which are close together than lots of states which are vaguely close - maybe I should look into a potential transform on the SBERT similarities to play better with state fidelity.